{
  "id": null,
  "uid": "payflow-standard-monitoring",
  "title": "Payflow Standard Monitoring",
  "timezone": "browser",
  "schemaVersion": 38,
  "version": 2,
  "refresh": "30s",
  "tags": ["payflow", "standard", "golden-signals"],
  "templating": {
    "list": [
      {
        "name": "namespace",
        "label": "Namespace",
        "type": "query",
        "datasource": "Prometheus",
        "query": "label_values(up, namespace)",
        "current": { "text": "payflow", "value": "payflow" }
      },
      {
        "name": "service",
        "label": "Service",
        "type": "query",
        "datasource": "Prometheus",
        "query": "label_values(up{namespace=\"$namespace\"}, service)",
        "includeAll": true,
        "allValue": ".*",
        "current": { "text": "All", "value": ".*" }
      }
    ]
  },
  "panels": [
    {
      "type": "stat",
      "title": "Targets Up",
      "description": "How many discovered scrape targets are currently healthy (up == 1).",
      "datasource": "Prometheus",
      "targets": [
        { "expr": "sum(up{namespace=\"$namespace\",service=~\"$service\"} == 1)" }
      ],
      "gridPos": { "x": 0, "y": 0, "w": 6, "h": 4 }
    },
    {
      "type": "stat",
      "title": "Targets Down",
      "description": "How many discovered scrape targets are failing (up == 0).",
      "datasource": "Prometheus",
      "targets": [
        { "expr": "sum(up{namespace=\"$namespace\",service=~\"$service\"} == 0)" }
      ],
      "gridPos": { "x": 6, "y": 0, "w": 6, "h": 4 }
    },
    {
      "type": "stat",
      "title": "Pods Ready",
      "description": "Count of containers in ready state for the selected namespace.",
      "datasource": "Prometheus",
      "targets": [
        { "expr": "sum(kube_pod_container_status_ready{namespace=\"$namespace\",condition=\"true\"})" }
      ],
      "gridPos": { "x": 12, "y": 0, "w": 6, "h": 4 }
    },
    {
      "type": "stat",
      "title": "Pod Restarts (1h)",
      "description": "Container restarts in the last hour; rising values indicate instability.",
      "datasource": "Prometheus",
      "targets": [
        { "expr": "sum(increase(kube_pod_container_status_restarts_total{namespace=\"$namespace\"}[1h]))" }
      ],
      "gridPos": { "x": 18, "y": 0, "w": 6, "h": 4 }
    },
    {
      "type": "timeseries",
      "title": "Request Rate by Service (RPS)",
      "description": "Golden signal: traffic. Uses counter rate over 5 minutes, grouped by service.",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "sum by (service) (rate(http_requests_total{namespace=\"$namespace\",service=~\"$service\"}[5m]))",
          "legendFormat": "{{service}}"
        }
      ],
      "gridPos": { "x": 0, "y": 4, "w": 12, "h": 8 }
    },
    {
      "type": "timeseries",
      "title": "Error Rate by Service (%)",
      "description": "Golden signal: errors. Percent of 5xx requests over total requests by service.",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "100 * (sum by (service) (rate(http_requests_total{namespace=\"$namespace\",service=~\"$service\",status_code=~\"5..\"}[5m])) / clamp_min(sum by (service) (rate(http_requests_total{namespace=\"$namespace\",service=~\"$service\"}[5m])), 0.001))",
          "legendFormat": "{{service}}"
        }
      ],
      "gridPos": { "x": 12, "y": 4, "w": 12, "h": 8 }
    },
    {
      "type": "timeseries",
      "title": "Latency p95 by Service (seconds)",
      "description": "Golden signal: latency. 95th percentile request duration from histogram buckets.",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum by (le, service) (rate(http_request_duration_seconds_bucket{namespace=\"$namespace\",service=~\"$service\"}[5m])))",
          "legendFormat": "{{service}}"
        }
      ],
      "gridPos": { "x": 0, "y": 12, "w": 12, "h": 8 }
    },
    {
      "type": "timeseries",
      "title": "CPU Usage by Pod (cores)",
      "description": "Golden signal: saturation. CPU cores consumed per pod over 5-minute window.",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "sum by (pod) (rate(container_cpu_usage_seconds_total{namespace=\"$namespace\",container!=\"\",pod!=\"\"}[5m]))",
          "legendFormat": "{{pod}}"
        }
      ],
      "gridPos": { "x": 12, "y": 12, "w": 12, "h": 8 }
    },
    {
      "type": "timeseries",
      "title": "Memory Working Set by Pod (MiB)",
      "description": "Memory pressure view per pod; values are converted from bytes to MiB.",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "sum by (pod) (container_memory_working_set_bytes{namespace=\"$namespace\",container!=\"\",pod!=\"\"}) / 1024 / 1024",
          "legendFormat": "{{pod}}"
        }
      ],
      "gridPos": { "x": 0, "y": 20, "w": 12, "h": 8 }
    },
    {
      "type": "timeseries",
      "title": "CPU Throttling by Pod (%)",
      "description": "Shows how often CPU was throttled; sustained values indicate limits are too low.",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "100 * (sum by (pod) (rate(container_cpu_cfs_throttled_periods_total{namespace=\"$namespace\",container!=\"\",pod!=\"\"}[5m])) / clamp_min(sum by (pod) (rate(container_cpu_cfs_periods_total{namespace=\"$namespace\",container!=\"\",pod!=\"\"}[5m])), 0.001))",
          "legendFormat": "{{pod}}"
        }
      ],
      "gridPos": { "x": 12, "y": 20, "w": 12, "h": 8 }
    },
    {
      "type": "logs",
      "title": "Namespace Logs (Loki)",
      "description": "Correlate metric anomalies with logs from the same namespace.",
      "datasource": "Loki",
      "targets": [
        { "expr": "{namespace=\"$namespace\"}" }
      ],
      "options": {
        "showLabels": true,
        "showTime": true,
        "sortOrder": "Descending"
      },
      "gridPos": { "x": 0, "y": 28, "w": 24, "h": 10 }
    },
    {
      "type": "timeseries",
      "title": "Node CPU Usage (%)",
      "description": "Node-level CPU usage from node-exporter.",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "100 * (1 - avg by (instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[5m])))",
          "legendFormat": "{{instance}}"
        }
      ],
      "gridPos": { "x": 0, "y": 38, "w": 12, "h": 8 }
    },
    {
      "type": "timeseries",
      "title": "Node Memory Usage (%)",
      "description": "Node-level memory utilization from node-exporter.",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "100 * (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes))",
          "legendFormat": "{{instance}}"
        }
      ],
      "gridPos": { "x": 12, "y": 38, "w": 12, "h": 8 }
    },
    {
      "type": "timeseries",
      "title": "CPU Usage as % of Limits by Pod",
      "description": "Pod CPU usage divided by configured CPU limits.",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "100 * (sum by (pod) (rate(container_cpu_usage_seconds_total{namespace=\"$namespace\",container!=\"\",pod!=\"\"}[5m])) / clamp_min(sum by (pod) (kube_pod_container_resource_limits{namespace=\"$namespace\",resource=\"cpu\"}), 0.001))",
          "legendFormat": "{{pod}}"
        }
      ],
      "gridPos": { "x": 0, "y": 46, "w": 12, "h": 8 }
    },
    {
      "type": "timeseries",
      "title": "Memory Usage as % of Limits by Pod",
      "description": "Pod memory working set divided by configured memory limits.",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "100 * (sum by (pod) (container_memory_working_set_bytes{namespace=\"$namespace\",container!=\"\",pod!=\"\"}) / clamp_min(sum by (pod) (kube_pod_container_resource_limits{namespace=\"$namespace\",resource=\"memory\"}), 1))",
          "legendFormat": "{{pod}}"
        }
      ],
      "gridPos": { "x": 12, "y": 46, "w": 12, "h": 8 }
    },
    {
      "type": "bargauge",
      "title": "Top 5 CPU Consumers (cores)",
      "description": "Highest CPU-consuming pods in the namespace.",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "topk(5, sum by (pod) (rate(container_cpu_usage_seconds_total{namespace=\"$namespace\",container!=\"\",pod!=\"\"}[5m])))",
          "legendFormat": "{{pod}}"
        }
      ],
      "gridPos": { "x": 0, "y": 54, "w": 12, "h": 8 }
    },
    {
      "type": "bargauge",
      "title": "Top 5 Memory Consumers (MiB)",
      "description": "Highest memory-consuming pods in the namespace.",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "topk(5, sum by (pod) (container_memory_working_set_bytes{namespace=\"$namespace\",container!=\"\",pod!=\"\"}) / 1024 / 1024)",
          "legendFormat": "{{pod}}"
        }
      ],
      "gridPos": { "x": 12, "y": 54, "w": 12, "h": 8 }
    },
    {
      "type": "timeseries",
      "title": "CPU Usage by Deployment (cores)",
      "description": "Deployment-level CPU breakdown inferred from pod naming.",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "sum by (deployment) (label_replace(rate(container_cpu_usage_seconds_total{namespace=\"$namespace\",container!=\"\",pod!=\"\"}[5m]), \"deployment\", \"$1\", \"pod\", \"^(.*)-[a-z0-9]{9,10}-[a-z0-9]{5}$\"))",
          "legendFormat": "{{deployment}}"
        }
      ],
      "gridPos": { "x": 0, "y": 62, "w": 12, "h": 8 }
    },
    {
      "type": "timeseries",
      "title": "Memory Usage by Deployment (MiB)",
      "description": "Deployment-level memory breakdown inferred from pod naming.",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "sum by (deployment) (label_replace(container_memory_working_set_bytes{namespace=\"$namespace\",container!=\"\",pod!=\"\"} / 1024 / 1024, \"deployment\", \"$1\", \"pod\", \"^(.*)-[a-z0-9]{9,10}-[a-z0-9]{5}$\"))",
          "legendFormat": "{{deployment}}"
        }
      ],
      "gridPos": { "x": 12, "y": 62, "w": 12, "h": 8 }
    },
    {
      "type": "stat",
      "title": "Firing Alerts",
      "description": "Current count of firing alerts in selected namespace.",
      "datasource": "Prometheus",
      "targets": [
        { "expr": "sum(ALERTS{namespace=\"$namespace\",alertstate=\"firing\"})" }
      ],
      "gridPos": { "x": 0, "y": 70, "w": 6, "h": 4 }
    },
    {
      "type": "stat",
      "title": "Pending Alerts",
      "description": "Current count of pending alerts in selected namespace.",
      "datasource": "Prometheus",
      "targets": [
        { "expr": "sum(ALERTS{namespace=\"$namespace\",alertstate=\"pending\"})" }
      ],
      "gridPos": { "x": 6, "y": 70, "w": 6, "h": 4 }
    },
    {
      "type": "bargauge",
      "title": "Firing Alerts by Name",
      "description": "Which alert rules are currently firing.",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "sum by (alertname) (ALERTS{namespace=\"$namespace\",alertstate=\"firing\"})",
          "legendFormat": "{{alertname}}"
        }
      ],
      "gridPos": { "x": 12, "y": 70, "w": 12, "h": 4 }
    }
  ]
}
